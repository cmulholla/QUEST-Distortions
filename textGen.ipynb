{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\meaps\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:20<00:00, 40.19s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "from accelerate import Accelerator\n",
    "from peft import prepare_model_for_kbit_training, prepare_model_for_int8_training\n",
    "\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "#model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model_id = \"teknium/OpenHermes-2-Mistral-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "precision = \"fp4\"\n",
    "path=f\"N:\\\\AI\\\\text-generation-webui-main\\\\models\\\\teknium_OpenHermes-2-Mistral-7B\\\\\"\n",
    "#path=f\"N:\\\\AI\\\\mistral-7B-instruct\\\\\"\n",
    "\n",
    "# if the model variable exists, delete it to free up memory before loading the new model\n",
    "if 'model' in locals():\n",
    "    model = None\n",
    "\n",
    "if (precision == \"fp16\"):\n",
    "    model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.float16).to(\"cuda\")\n",
    "elif (precision == \"fp8\"):\n",
    "    model = AutoModelForCausalLM.from_pretrained(path, load_in_8bit=True, device_map='cuda')\n",
    "elif (precision == \"fp4\"):\n",
    "    model = AutoModelForCausalLM.from_pretrained(path, load_in_4bit=True, device_map='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input text\n",
    "additional_context = \"[INST] The following is a conversation with an AI assistant. The assistant is helpful and concise. The assistant does not respond to the question, and only does as the question says. [/INST]\"\n",
    "input_text = \"Please generate a single sentence with only the \\\"Mental Filters\\\" cognitive distortion, from the perspective of the person with the distortion.\"\n",
    "\n",
    "# Generate outputs\n",
    "def generate_response(text):\n",
    "    try:\n",
    "        # Append a prompt to the user's input\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=512, do_sample=True, use_cache=True, top_k=40, top_p=0.1, temperature=0.7, repetition_penalty=1.2, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id, bos_token_id=tokenizer.bos_token_id)\n",
    "        responseIn = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        #text = responseIn + \"\\n[INST] Please generate another sentence. [/INST]\"\n",
    "        #inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        #outputs = model.generate(**inputs, max_new_tokens=512, do_sample=True, use_cache=True, top_k=40, top_p=0.1, temperature=0.7, repetition_penalty=1.2, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id, bos_token_id=tokenizer.bos_token_id)\n",
    "        #responseIn = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Remove the prompt from the start of the response\n",
    "        response = responseIn[len(text):]\n",
    "    except Exception as e:\n",
    "        response = \"Sorry, I encountered an error. Please try again.\"\n",
    "        print(e)\n",
    "    return response\n",
    "\n",
    "# Generate a response\n",
    "#outputs = generate_response(additional_context+\"\\n\"+\"[INST] \" + input_text + \" [/INST]\")\n",
    "\n",
    "#for i in range(5):\n",
    "#    outputs = generate_response(outputs+\"\\n\"+\"[INST] \" + \"Please generate another sentence with only the \\\"Mental Filters\\\" cognitive distortion.\" + \" [/INST]\")\n",
    "\n",
    "# Decode the generated outputs\n",
    "#print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "generate_data = [\n",
    "                    (\"Magnification\", 5), \n",
    "                    (\"Labeling\", 5)\n",
    "                ]\n",
    "\n",
    "# Create a DataFrame from the history list\n",
    "df = pd.DataFrame([], columns=[\"Distorted part\",\"Dominant Distortion\"])\n",
    "\n",
    "for c in range(len(generate_data)):\n",
    "\n",
    "    # Generate a response\n",
    "    main_input = additional_context+\"\\n\"+\"[INST] \" + input_text + \" [/INST]\"\n",
    "    output = generate_response(main_input)\n",
    "\n",
    "\n",
    "    # Create a list to store inputs and outputs\n",
    "    history = []\n",
    "\n",
    "    # Add the initial input and output to the history\n",
    "    history.append((main_input, output))\n",
    "\n",
    "    for i in range(generate_data[c][1]):\n",
    "        # Generate a new response\n",
    "        input_string = \"\"\n",
    "        for j in range(len(history)):\n",
    "            input_string += history[j][0] + \"\\n\" + history[j][1] + \"\\n\"\n",
    "        input_string = input_string + \"[INST] \" + \"Please generate another completely new sentence with only the \\\"\" + generate_data[c][0] + \"\\\" cognitive distortion.\" + \" [/INST]\\n\"\n",
    "\n",
    "        if input_string.__len__() > 8192:\n",
    "            # Remove the string from the start of the end of the first string\n",
    "            input_string = input_string[0:main_input.__len__()] + input_string[input_string.find(\"[INST]\", main_input.__len__()+(input_string.__len__() - 8192)):]\n",
    "\n",
    "\n",
    "        new_output = generate_response(input_string)\n",
    "\n",
    "        if new_output[0] == '\\n':\n",
    "            new_output = new_output[1:]\n",
    "\n",
    "\n",
    "        # Add the new input and output to the history\n",
    "        history.append((\"[INST] Please generate another completely new sentence with only the \\\"\" + generate_data[c][0] + \"\\\" cognitive distortion. [/INST]\", new_output))\n",
    "\n",
    "        # Concatenate the new row to the DataFrame\n",
    "        df = pd.concat([df, pd.DataFrame({\"Distorted part\": [new_output], \"Dominant Distortion\": [generate_data[c][0]]})], ignore_index=True)\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "df.to_csv(\"distorted_parts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# function to generate a record based on 3 random records from the original data\n",
    "def generate_record(original_data, input_text, distortion):\n",
    "    distorion_data = original_data[original_data[\"Dominant Distortion\"] == distortion]\n",
    "    random_indices = np.random.choice(len(distorion_data), 3, replace=False)\n",
    "\n",
    "    # Get the 3 random records\n",
    "    rand_records = []\n",
    "    for index in random_indices:\n",
    "        rand_records.append(distorion_data.iloc[index][\"Distorted part\"])\n",
    "\n",
    "    # Generate a response\n",
    "    main_input = additional_context+\"\\n\"+\"[INST] \" + input_text + \" [/INST]\\n\" + rand_records[0] + \"\\n\" + rand_records[1] + \"\\n\" + rand_records[2] + \"\\n\"\n",
    "    print(main_input)\n",
    "\n",
    "    output = generate_response(main_input)\n",
    "\n",
    "    if len(output) == 0:\n",
    "        output = \"Sorry, I encountered an error. Please try again.\"\n",
    "\n",
    "    if output[0] == '\\n':\n",
    "        output = output[1:]\n",
    "\n",
    "    return generate_response(main_input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.351111111111111\n"
     ]
    }
   ],
   "source": [
    "pasta = [8+5+3.5+2+2+0.16+8+6.5+4]\n",
    "pasta_people = [(\"me\", 8), \n",
    "                (\"josh f\", 5), \n",
    "                (\"josh a\", 3.5), \n",
    "                (\"rachel\", 2), \n",
    "                (\"emma\", 0.16), \n",
    "                (\"daniel\", 8), \n",
    "                (\"her\", 2), \n",
    "                (\"katrina\", 6.5),\n",
    "                (\"nick\", 4)]\n",
    "\n",
    "print(sum(pasta)/len(pasta_people))\n",
    "\n",
    "#me, josh f, josh a, her, rachel, emma, daniel, katrina\n",
    "#                (\"her\", 2), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] The following is a conversation with an AI assistant. The assistant is helpful and concise. The assistant does not respond to the question, and only does as the question says. [/INST]\n",
      "[INST] Please generate four similar sentences with only the \"Mental Filters\" cognitive distortion within it, from the perspective of the person with the distortion. [/INST]\n",
      "It’s like when I hit 20, something inside me went off and it led to a series of events of which the results still really resonate with me, and I haven’t really had a chance to really get someone else’s perspective. I think something that didn’t help is that I developed a really unhealthy obsession with J.D. Salinger’s The Catcher in The Rye during this time. I think it honestly had a negative effect on me, despite my deep love for the book\n",
      "I literally see a mental picture of myself being punched, slapped, abused, shot, decapitated, multilated, head crushed with a building, etc. I feel deeply offended when criticized, I don’t “hang out” with my peers and feel inferior to them, never been to a party or other social gathering, I feel others are always judging me and they remember every mistake I’ve ever made even though it’s illogical to think so, I’ve never had a girlfriend, I find it hard to trust others, and my only escape is fantasies I indulge in while I’m alone.\n",
      "He claims he’s severely depressed and has outbursts a couple days per week, sometimes more. During these outbursts he rants about his mental condition, attacks my and my families insecurities and lifestyles, and even sometimes becomes violent over them. He even threatens suicide on more serious outbursts.\n",
      "\n",
      "The way she talks makes her sound like she's trying too hard to be cool, but at the same time, she seems to have no idea how to act around people who aren't her friends. She also tends to make fun of people behind their backs, especially if they're not part of her group.\n",
      "EOF\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Annotated_data.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "# print the distortion types\n",
    "# print(data[\"Dominant Distortion\"].unique())\n",
    "\n",
    "inputText = \"Please generate four similar sentences with only the \\\"Mental Filters\\\" cognitive distortion within it, from the perspective of the person with the distortion.\"\n",
    "distortion = \"Mental filter\"\n",
    "\n",
    "print(generate_record(data, inputText, distortion) + \"\\nEOF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
