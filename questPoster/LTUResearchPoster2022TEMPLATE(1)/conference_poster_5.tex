%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% a0poster Landscape Poster
% LaTeX Template
% Version 1.0 (22/06/13)
%
% The a0poster class was created by:
% Gerlinde Kettl and Matthias Weiser (tex@kettl.de)
% 
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a0,landscape]{a0poster}
\usepackage{multicol} % This is so we can have multiple columns of text side-by-side
\columnsep=100pt % This is the amount of white space between the columns in the poster
\renewcommand{\columnseprulecolor}{\color{LightBlue}}
\columnseprule=3pt % This is the thickness of the black line between the columns in the poster

\usepackage[svgnames]{xcolor} % Specify colors by their 'svgnames', for a full list of all colors available see here: http://www.latextemplates.com/svgnames-colors

\usepackage{times} % Use the times font
%\usepackage{palatino} % Uncomment to use the Palatino font

\usepackage{graphicx} % Required for including images
\graphicspath{{figures/}} % Location of the graphics files
\usepackage{booktabs} % Top and bottom rules for table
\usepackage[font=small,labelfont=bf]{caption} % Required for specifying captions to tables and figures
\usepackage{amsfonts, amsmath, amsthm, amssymb} % For math fonts, symbols and environments
\usepackage{wrapfig} % Allows wrapping text around tables and figures

\begin{document}

\begin{minipage}[b]{0.55\linewidth}
\veryHuge \color{DarkOrange} \textbf{Exploring Generative A.I. for Addressing Class \\ Imbalance in Cognitive Distortion Predictive Models} 
\\
\color{Black}\\ % Title
%\Huge\textit{An Exploration of Complexity}\\[1cm] % Subtitle
\huge\\Connor Mulholland, BSCS Candidate; Paula Lauren, PhD, Professor, CoAS % Author(s)

 %Lawrence Technological University\\ % University/organization
\end{minipage}
%
\begin{minipage}[b]{0.25\linewidth}
\color{DarkSlateGray}\Large \textbf{Contact Information:}\\
Dept.\ of Mathematics and Computer Science\\ % Address
Science Building S121D\\
\\

\end{minipage}
%
\begin{minipage}[b]{0.18\linewidth}
\includegraphics[width=20cm]{LTU2.png} % Logo or a photo of you, adjust its dimensions here
\end{minipage}

\vspace{1cm} % A bit of extra whitespace between the header and poster content

%----------------------------------------------------------------------------------------

\begin{multicols}{4} % This is how many columns your poster will be broken into, a poster with many figures may benefit from less columns whereas a text-heavy poster benefits from more



\color{Teal}
\section*{Abstract}
\color{black} 
Modern tools for natural language generation may enable Artificial Intelligence (AI) models to be better trained on imbalanced data by generating records for the minority classes. Specifically, the psychoanalytic cognitive distortion data is based on the irrational or biased ways of thinking which can contribute to negative emotions and behaviors, which are crucial in many types of therapy. We compare various types of Generative AI models for generating new records and the current limitations of these new models. We find that pretrained Sentence-Bidirectional Encoder Representations from Transformers (Sentence-BERT) embeddings (i.e., multilingual-e5-large-instruct) used to train a Support Vector Machine (SVM) classifier model yields the best binary results with an F1-score of 0.756. The addition of generated data using the Mistral-7B-Instruct-v0.2 model with recursive data generation on the binary classification task resulted in a marginal boost in performance with an F1-score of 0.765 using the same Sentence-BERT embeddings with SVM classification model.



%----------------------------------------------------------------------------------------
%	OBJECTIVES
%----------------------------------------------------------------------------------------

\color{Teal} % DarkSlateGray color for the rest of the content

%----------------------------------------------------------------------------------------
%	MATERIALS AND METHODS
%----------------------------------------------------------------------------------------



\color{Black}

\begin{center}\vspace{0.5cm}
\includegraphics[width=0.99\linewidth]{GPT_1.png}
\captionof{figure}{\color{Teal} Generative Pre-trained Transformer Architecture.}
\end{center}\vspace{0.5cm}

\color{Teal}

\section*{Introduction}
\color{Black}

Cognitive distortions are irrational or biased ways of thinking that can contribute to negative emotions and behaviors. Cognitive-behavioral therapy (CBT) is a common therapeutic approach that aims to help individuals identify and challenge these distortions. Detecting cognitive distortions from patient-therapist interactions is a challenging task that has been addressed in recent research. The original paper by \cite{original_paper} uses a dataset of patient-therapist interactions to train a model to detect cognitive distortions. However, the dataset is highly imbalanced, with the majority of the data belonging to the non-distorted class. This class imbalance poses a challenge for training accurate predictive models. Generative AI models have the potential to address class imbalance by generating synthetic data for the minority classes. In this study, we explore the use of generative AI models to address class imbalance in cognitive distortion predictive models. We compare the performance of different generative AI models and classification algorithms on the task of detecting cognitive distortions from patient-therapist interactions. We consider both binary and multi-class classification tasks and evaluate the models using F1-score as the performance metric.

\begin{center}\vspace{0.5cm}
\includegraphics[width=0.99\linewidth]{Example.png}
\captionof{figure}{\color{Teal} Analogy Pair Task Example}
\end{center}\vspace{0.5cm}

\color{Teal}
\section*{Initial Data Analysis}
\color{Black}

\color{Teal}
\section*{Sentence-BERT Embeddings}
\color{Black}


\begin{center}\vspace{1cm}
\includegraphics[width=0.99\linewidth]{Example1.png}
\captionof{figure}{\color{Teal} GPT Analogy Pair Task Result}
\end{center}\vspace{1cm}

\color{Black}

%\color{Teal}

%\section*{Comparison of Subword Tokens}
%\color{Black}
\color{Teal}
\section*{Generative AI Models}
\color{Black}


\begin{center}
\includegraphics[width=0.60\linewidth]{Fine-Tune-GPT.png}
\captionof{figure}{\color{Teal} Fine Tuning Architecture}
\end{center}\vspace{1cm}

\color{Teal}
\subsection*{The Fine-Tuning Process}
\begin{center}
\includegraphics[width=0.99\linewidth]{FlowChart.png}
\captionof{figure}{\color{Teal} Fine-Tuning Process Step-by-Step}
\end{center}\vspace{1cm}

\color{black}
The below illustration depicts the outcome of the fine-tuned model for the example elaborated in Figure 3.

\begin{center}\vspace{1cm}
\includegraphics[width=0.99\linewidth]{Example2.png}
\captionof{figure}{\color{Teal} Fine-Tuned Model Analogy Pair Test Result}
\end{center}\vspace{1cm}

Bananas are more closely related to avocados than to limes because both belong to the same plant family, called the Musaceae family.
\vspace{0.5cm}

As illustrated in Figure 6, fine-tuning a pre-trained language model can enhance its performance on a given task or dataset. By adjusting the pre-trained model on a specific dataset, the model can learn the nuances and subtleties of the data and perform more accurately and meaningfully.
\vspace{0.5cm}

In addition to the example shown in Figure 6, there are several other examples where fine-tuning has demonstrated impressive results. These instances are shown in the table below.

 \begin{center}\vspace{1cm}
\centering
\begin{tabular}{c|c|r}
\toprule
\textbf{Prompt}  &  \textbf{GPT}  & \textbf{Fine-Tuned} \\
\midrule
Yellow is to banana and green is to  &    Lime  & Avocado\\ \hline
Computer is to software and brain is to   &   Hardware   & Cognition\\ \hline
Knife is to cooking as screwdriver is to  &   DIY projects  & Repair \\ \hline
Shovel is to gardening as keyboard is to &   Computing   & Typing\\ \hline
Microphone is to sound as camera is to &    Light  & Images\\ \hline
Car is to engine as computer is to &   Keyboard   & Processor\\ \hline
Glacier is to iceberg as stream is to &   Rock   & River \\ \hline
Rust is to decay as charcoal is to &   Burning   & Ash\\ \hline
Forgiveness is to healing as medicine is to &   Health    & Recovery \\ \hline
\bottomrule
\end{tabular}
\captionof{table}{\color{Teal} {  Analysis of Fine-Tuned Model }
\label{origCounts}}
\end{center}\vspace{1cm}


\color{Teal}
\section*{F1-Score Results Analysis}
\color{Black}


\begin{itemize}
  \item The challenge at hand is that OpenAI, a leading AI research organization, will be offering a limited-time free subscription to their API service. This subscription will only be available for a duration of three months and will come with a limited amount of credits for using the API and fine-tuning it. Users may need to carefully plan and prioritize their use of the API within the allotted time and credit. 
  \item During the process of fine-tuning a learning model, users may have to wait for an unpredictable amount of time for the model to be fully trained. This wait time can vary depending on several unknown factors, and can range from minutes to hours or even days. This unpredictability can be a frustrating and time-consuming challenge for users who need to balance their fine-tuning activities with other tasks and responsibilities.
\end{itemize}



\color{Teal}
\section*{Conclusion}
\color{Black}



In conclusion to our research, fine-tuning is a crucial process that can significantly improve the performance of pre-trained NLP models on specific tasks or domains. Our findings show that the pre-trained GPT models can benefit from fine-tuning, as they are trained on massive datasets with broad contexts and may not generalize well on specific tasks or domains. Fine-tuning allows the models to adapt to the target task or domain by learning the relevant patterns and relationships from a smaller dataset, leading to higher accuracy and efficiency.

\vspace{1cm}
Our analysis of the fine-tuning results shows that the fine-tuned GPT models outperformed their pre-trained large language model and achieved better results in the analogy task. This indicates the effectiveness of the fine-tuning process in improving the models' performance on specific tasks or domains.

\color{Teal}
\section*{Future work}
\color{Black}


As a next step in this area, one potential direction for future work is to explore the potential of fine-tuning NLP models for mathematical tasks. Pre-trained models have shown impressive performance in language-related tasks, but their performance in mathematical tasks is relatively limited. To overcome this limitation, researchers can fine-tune models specifically for mathematical tasks, such as equation solving or mathematical expression generation. This could involve designing new architectures or adapting existing ones to better handle mathematical inputs and outputs, as well as experimenting with different fine-tuning techniques to optimize the models' performance on these tasks.


Another area for future work is investigating the feasibility of creating customized language models tailored to specific tasks or domains, rather than relying on pre-trained models. This could involve collecting and preprocessing relevant data, designing appropriate architectures, and training the models from scratch using various supervised or unsupervised learning techniques. By creating language models specific to a given task or domain, researchers can build more efficient and accurate models that can better handle the specific nuances of the input data.
%----------------------------------------------------------------------------------------
%	FORTHCOMING RESEARCH
%----------------------------------------------------------------------------------------

%\section*{Forthcoming Research}

%Vivamus molestie, risus tempor vehicula mattis, libero arcu volutpat purus, sed blandit sem nibh eget turpis. Maecenas rutrum dui blandit lorem vulputate gravida. Praesent venenatis mi vel lorem tempor at varius diam sagittis. Nam eu leo id turpis interdum luctus a sed augue. Nam tellus.

 %----------------------------------------------------------------------------------------
%	REFERENCES
%----------------------------------------------------------------------------------------

%\nocite{*} % Print all references regardless of whether they were cited in the poster or not
%\bibliographystyle{plain} % Plain referencing style
%\bibliography{sample} % Use the example bibliography file sample.bib

%----------------------------------------------------------------------------------------
\nocite{*}
\end{multicols}
\end{document}